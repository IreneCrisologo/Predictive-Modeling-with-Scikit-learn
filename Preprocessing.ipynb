{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "- What is preprocessing and how it is done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.preprocessing\n",
    "\n",
    "In simple words, pre-processing refers to the transformations applied to your data before feeding it to the algorithm \n",
    "\n",
    "Scikit-learn library has a pre-built functionality under **sklearn.preprocessing** that we will explore in this module\n",
    "\n",
    "## Train-Test split\n",
    "\n",
    "Before working on anything we need to make sure that we set a portion of data aside to be able to say anything about what we can do with the unseen data \n",
    "\n",
    "A core practice in machine learning is to split the dataset into diffent partitions for training and testing\n",
    "\n",
    "Scikit-learn has a convenient method to assist in that process:\n",
    "\n",
    "train_test_split(sample, response, test_size=0.25, shuffle=True)\n",
    "\n",
    "The split size is controlled using the attribute test_size. By default, test_size is set to 25% of the dataset size. It is standard practice to shuffle the dataset before splitting by setting the attribute shuffle=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split in train and test sets\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, shuffle=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling or not scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is a monotonic transformation - the relative order of smaller to larger value in a variable is maintained post the scaling.\n",
    "\n",
    "Normalization and Standardization are explained below\n",
    "\n",
    "Algorithms that do not require normalization/scaling are the ones that rely on rules. They would not be affected by any monotonic transformations of the variables.  e.g. CART, Random Forests, Gradient Boosted Decision Trees etc. \n",
    "\n",
    "Also, Algorithms that rely on distributions of the variables, like Naive Bayes also do not need scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization transforms the features into a Standard Gaussian (or normal) distribution with a mean of 0 and standard deviation of 1\n",
    "\n",
    "Standardization is used when algorithm requires computation of distance (Euclidean) to avoid large scale features dominating others (e.g. KNN, K-means, Minimum distance classifier)\n",
    "\n",
    "It matters in PCA to avoid bias towards high magnitude features. For gradient descent based algorithm, feature scaling helps in faster convergence, in SVM it can reduce the time to find support vectors.\n",
    "\n",
    "Scikit-learn implements data standardization in the StandardScaler module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "standardize_Xtrain = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9, 2.4, 3.3, 1. ],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [5.1, 3.8, 1.5, 0.3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1071133 , -1.55550783, -0.19828204, -0.18186407],\n",
       "       [ 0.20818428, -2.02008617,  0.18745771, -0.18186407],\n",
       "       [ 0.5669018 , -1.32321866,  0.68340883,  0.46271743],\n",
       "       [ 0.5669018 , -0.85864032,  0.68340883,  0.84946633],\n",
       "       [-0.86796829,  1.69654054, -1.19018428, -1.08427816]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardize_Xtrain[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Standardize X_test and print how first 5 rows of X_test and standardize_Xtest look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization transforms the features in the dataset so that it has a unit norm or has magnitude or length of 1 \n",
    "\n",
    "The length of a vector is the square-root of the sum of squares of the vector elements \n",
    "\n",
    "A unit vector (or unit norm) is obtained by dividing the vector by its length \n",
    "\n",
    "Note: Normalizing the dataset is particularly useful in scenarios where the dataset is sparse (i.e., a large number of observations are zeros) and also have differing scales. \n",
    "\n",
    "Normalization in Scikit-learn is implemented in the Normalizer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X_train)\n",
    "normalize_Xtrain = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9, 2.4, 3.3, 1. ],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [5.1, 3.8, 1.5, 0.3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75916547, 0.37183615, 0.51127471, 0.15493173],\n",
       "       [0.78892752, 0.28927343, 0.52595168, 0.13148792],\n",
       "       [0.74143307, 0.29421947, 0.57667016, 0.17653168],\n",
       "       [0.73122464, 0.31338199, 0.56873028, 0.20892133],\n",
       "       [0.77964883, 0.58091482, 0.22930848, 0.0458617 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_Xtrain[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Normalize X_test and print how first 5 rows of X_test and normalize_Xtest look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding or Encoding Categorical Variables\n",
    "\n",
    "https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization\n",
    "\n",
    "Encoding categorical variables is the technique for converting non-numerical features with labels into a numerical representation for use in machine learning modeling \n",
    "\n",
    "Scikit-learn provides modules for encoding categorical variables including the LabelEncoder, for encoding labels as integers \n",
    "\n",
    "LabelEncoder is typically used on the target variable to transform a vector of hashable categories (or labels) into an integer representation by encoding label with values between 0 and the number of categories minus 1. This is further illustrated in Figure\n",
    "\n",
    "![LabelEncoder](LabelEncoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['5', '8', 'calabar'],\n",
       "       ['9', '3', 'uyo'],\n",
       "       ['8', '6', 'owerri'],\n",
       "       ['0', '5', 'uyo'],\n",
       "       ['2', '3', 'calabar'],\n",
       "       ['0', '8', 'calabar'],\n",
       "       ['1', '8', 'owerri']], dtype='<U11')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create dataset\n",
    "LabelEncodingdata = np.array([[5,8,\"calabar\"],[9,3,\"uyo\"],[8,6,\"owerri\"],\n",
    "                    [0,5,\"uyo\"],[2,3,\"calabar\"],[0,8,\"calabar\"],\n",
    "                    [1,8,\"owerri\"]])\n",
    "LabelEncodingdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['5', '8', '0'],\n",
       "       ['9', '3', '2'],\n",
       "       ['8', '6', '1'],\n",
       "       ['0', '5', '2'],\n",
       "       ['2', '3', '0'],\n",
       "       ['0', '8', '0'],\n",
       "       ['1', '8', '1']], dtype='<U11')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate features and target\n",
    "X = LabelEncodingdata[:,:2]\n",
    "y = LabelEncodingdata[:,-1]\n",
    "\n",
    "# encode y\n",
    "encoder = LabelEncoder()\n",
    "encode_y = encoder.fit_transform(y)\n",
    "\n",
    "# adjust dataset with encoded targets\n",
    "LabelEncodingdata[:,-1] = encode_y\n",
    "LabelEncodingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Missing Data\n",
    "\n",
    "It is often the case that a dataset contains several missing observations \n",
    "\n",
    "Scikit-learn implements the Imputer module for completing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5., nan,  8.],\n",
       "       [ 9.,  3.,  5.],\n",
       "       [ 8.,  6.,  4.],\n",
       "       [nan,  5.,  2.],\n",
       "       [ 2.,  3.,  9.],\n",
       "       [nan,  8.,  7.],\n",
       "       [ 1., nan,  5.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# create dataset\n",
    "Missing_data = np.array([[5,np.nan,8],[9,3,5],[8,6,4],\n",
    "                    [np.nan,5,2],[2,3,9],[np.nan,8,7],\n",
    "                    [1,np.nan,5]])\n",
    "Missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 8.],\n",
       "       [9., 3., 5.],\n",
       "       [8., 6., 4.],\n",
       "       [5., 5., 2.],\n",
       "       [2., 3., 9.],\n",
       "       [5., 8., 7.],\n",
       "       [1., 5., 5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute missing values - axix=0: impute along columns\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit_transform(Missing_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
